{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Data - Machine Learning\n",
    "\n",
    "The question that this project seeks to answer is whether macroeconomic factors at the time that a loan is originated have a measurable impact on the likelihood that borrowers will default. To evaluate this question, the impact of incorporating macroeconomic variables into Machine Learning models will be evaluated.\n",
    "\n",
    "First, the data will be cleaned so that it contains only relevant data <i>included in the dataset provided by Lending Club and known at the time that the loan is originated</i>. This will provide a baseline for the ability to predict loan outcomes without using external data. Second, logistic regression, SVM, and KNN will be used to evaluate how well the data can be used to predict loan defaults. Third, the data will be modified to include a number of macroeconomic features. Finally, the same machine learning methods will be employed on the modified data to evaluate whether adding macroeconomic features improves model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data/Library import\n",
    "\n",
    "The necessary libraries are imported. Also, the dataset to be used is imported from a .pkl file, which is available in the following shared Google Drive location (in .7z format):\n",
    "\n",
    "https://drive.google.com/file/d/1RCgoJYONVQJek5zrlShaGEzjrk99rAIk/view\n",
    "\n",
    "The data has been pre-processed to remove all loans that have not completed their entire term length. For example, loans that have 36-month terms are only included if they originated at least 36 months prior to the date that the data was updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import fredapi\n",
    "import datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Import the dataset containing completed loans\n",
    "df = pd.read_pickle(r'..\\data\\completed_loan_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Clean-Up\n",
    "\n",
    "The data includes some information that has was pulled in for previous analyses, including some macroeconomic variables. These are stripped for the initial analysis, which is intended to test only how well predictions using the data provided by Lending Club would perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a duplicate dataframe\n",
    "lc_data = df.copy()\n",
    "\n",
    "# From the duplicate dataframe, drop the columns that were not included in the original dataset\n",
    "lc_data = lc_data.drop(['bom_value', 'eom_value', 'month_avg', 'monthly_bks', 'lag_time', 'loan_end_date', 'loan_ended_flag',\n",
    "                       'sp_500_diff', 'unemployment_rate', 'date'], axis=1)\n",
    "\n",
    "# Remove all columns with too many null values. If more than 1% of the values in a column are null, then remove it\n",
    "lc_data = lc_data[[c for c in lc_data if lc_data[c].isnull().sum() <= .01 * len(lc_data)]]\n",
    "\n",
    "# Less than 0.2% of the remaining rows have null values. These are removed\n",
    "lc_data = lc_data.dropna()\n",
    "\n",
    "# For several of the columns, there is no reason to think that they should be a predictor of default rates or there\n",
    "# is no way that they are actionable prior to origination (such as issue date or collections recoveries). Additionally, the\n",
    "# interest rate is determined by grade and sub_grade, so grade and sub_grade are removed.\n",
    "lc_data = lc_data.drop(['id', 'url', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt',\n",
    "                        'debt_settlement_flag', 'grade', 'sub_grade', 'installment', 'loan_status', 'loan_amnt',\n",
    "                        'funded_amnt_inv', 'pymnt_plan', 'policy_code', 'application_type', 'out_prncp',\n",
    "                        'out_prncp_inv', 'last_credit_pull_d', 'hardship_flag', 'title', 'disbursement_method',\n",
    "                       'issue_d', 'earliest_cr_line', 'zip_code', 'addr_state', 'total_pymnt_inv', 'total_pymnt',\n",
    "                       'total_rec_late_fee', 'total_rec_prncp', 'total_rec_int', 'acc_now_delinq', 'delinq_amnt'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are printed below to evaluate what is left in the data. 'charge_off_flag' is the outcome variable - it is one if the loan charged off, and zero otherwise. The other features are datapoints known to Lending Club when the loan was originated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['funded_amnt', 'term', 'int_rate', 'home_ownership', 'annual_inc',\n",
       "       'verification_status', 'purpose', 'dti', 'delinq_2yrs',\n",
       "       'fico_range_low', 'fico_range_high', 'inq_last_6mths', 'open_acc',\n",
       "       'pub_rec', 'revol_bal', 'revol_util', 'total_acc',\n",
       "       'initial_list_status', 'last_fico_range_high', 'last_fico_range_low',\n",
       "       'collections_12_mths_ex_med', 'chargeoff_within_12_mths',\n",
       "       'pub_rec_bankruptcies', 'tax_liens', 'orig_month', 'charge_off_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the columns containing categories (\"object\" columns) are converted to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_data = pd.get_dummies(lc_data, columns=list(lc_data.select_dtypes(include=[\"object\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is ready to be split into X and y arrays, with X being all of the columns except for charge_off_flag and y being the charge_off_flag column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lc_data.drop(['charge_off_flag', 'orig_month'], axis=1).values\n",
    "y = lc_data['charge_off_flag'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X data is standardized so that datapoints of a greater scale do not have outsize influence on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = preprocessing.StandardScaler().fit_transform(Xtrain)\n",
    "Xtest = preprocessing.StandardScaler().fit_transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A KNN model with several different parameters is tested. The dataset is relatively sparse, so KNN is not expected to perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN training...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 174.7min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 268.6min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 433.7min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 555.2min\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed: 791.2min remaining: 71.9min\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 816.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on the train set...\n",
      "Predicting on the test set...\n"
     ]
    }
   ],
   "source": [
    "knn_parameters = {\"n_neighbors\": [3, 5], \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\"], \n",
    "               \"weights\": [\"uniform\", \"distance\"]}\n",
    "clf = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=knn_parameters, cv=3, n_jobs=-1, \n",
    "                   scoring=\"f1\", verbose=10)\n",
    "\n",
    "print('Starting KNN training...')\n",
    "clf.fit(Xtrain, ytrain)\n",
    "print('Predicting on the train set...')\n",
    "y_val_knn = clf.predict(Xtrain)\n",
    "print('Predicting on the test set...')\n",
    "y_test_knn = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix below indicates that the KNN model is useful for predicting defaults, even using only the variables in the Lending Club dataset. A commonly used statistic in lending is the proportion of non-defaulting borrowers who would be rejected based on a rule change for every defaulting borrower who is rejected (good-to-bad ratio). The good-to-bad ratio for this analasis is close to exactly one, meaning that rejecting would-be borrowers based on KNN predicting their default would result in one non-defaulting borrower rejected for every defaulting borrower rejected. This is almost certainly a good trade-off, as the value of a non-defaulted loan is typically much smaller than the cost of a default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "\n",
      "                     No Default Predicted  Default Predicted\n",
      "No Default Occurred                 69880               4121\n",
      "Default Occurred                     8117               4423\n",
      "\n",
      "Good-to-Bad ratio: 0.931720551661768\n"
     ]
    }
   ],
   "source": [
    "knn_cm_df = pd.DataFrame(confusion_matrix(ytest, y_test_knn))\n",
    "knn_cm_df.columns = ['No Default Predicted', 'Default Predicted']\n",
    "knn_cm_df['ind'] = ['No Default Occurred', 'Default Occurred']\n",
    "knn_cm_df = knn_cm_df.set_index('ind')\n",
    "knn_cm_df.index.name = None\n",
    "print('Confusion matrix: \\n\\n{}'.format(knn_cm_df))\n",
    "knn_gb_ratio = knn_cm_df.loc['No Default Occurred', 'Default Predicted'] / knn_cm_df.loc['Default Occurred', 'Default Predicted']\n",
    "print('\\nGood-to-Bad ratio: {}'.format(knn_gb_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, SVM is performed on the data, similarly with several specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVM training...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 57.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 85.9min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 157.7min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed: 241.4min remaining: 48.3min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed: 339.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on the train set...\n",
      "Predicting on the test set...\n"
     ]
    }
   ],
   "source": [
    "svc_parameters = {\"kernel\": [\"rbf\"], \"gamma\": [0.001, 0.0001], \"C\": [1, 10, 100, 1000]}\n",
    "clf = GridSearchCV(estimator=SVC(), param_grid=svc_parameters, cv=3, n_jobs=-1, \n",
    "                   scoring=\"f1\", verbose=10)\n",
    "\n",
    "print('Starting SVM training...')\n",
    "clf.fit(Xtrain, ytrain)\n",
    "print('Predicting on the train set...')\n",
    "y_val_svm = clf.predict(Xtrain)\n",
    "print('Predicting on the test set...')\n",
    "y_test_svm = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix below shows that SVM performed significantly better than KNN, accurately predicting approximately 2 defaults for every 1 false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "\n",
      "                     No Default Predicted  Default Predicted\n",
      "No Default Occurred                 71776               2225\n",
      "Default Occurred                     8314               4226\n",
      "\n",
      "Good-to-Bad ratio: 0.5265026029342168\n"
     ]
    }
   ],
   "source": [
    "svm_cm_df = pd.DataFrame(confusion_matrix(ytest, y_test_svm))\n",
    "svm_cm_df.columns = ['No Default Predicted', 'Default Predicted']\n",
    "svm_cm_df['ind'] = ['No Default Occurred', 'Default Occurred']\n",
    "svm_cm_df = svm_cm_df.set_index('ind')\n",
    "svm_cm_df.index.name = None\n",
    "print('Confusion matrix: \\n\\n{}'.format(svm_cm_df))\n",
    "gb_ratio = svm_cm_df.loc['No Default Occurred', 'Default Predicted'] / svm_cm_df.loc['Default Occurred', 'Default Predicted']\n",
    "print('\\nGood-to-Bad ratio: {}'.format(gb_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a random forest model is tested with several different specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest training...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed:   57.1s remaining:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on the train set...\n",
      "Predicting on the test set...\n"
     ]
    }
   ],
   "source": [
    "forest_parameters = {\"n_estimators\": [5, 10, 50], \"max_depth\": [2, 5, 7, 9]}\n",
    "clf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=forest_parameters, cv=3, n_jobs=-1, \n",
    "                   scoring=\"f1\", verbose=10)\n",
    "\n",
    "print('Starting Random Forest training...')\n",
    "clf.fit(Xtrain, ytrain)\n",
    "print('Predicting on the train set...')\n",
    "y_val_forest = clf.predict(Xtrain)\n",
    "print('Predicting on the test set...')\n",
    "y_test_forest = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model has results that are similar to, but very slightly worse than, the SVM results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "\n",
      "                     No Default Predicted  Default Predicted\n",
      "No Default Occurred                 71765               2236\n",
      "Default Occurred                     8481               4059\n",
      "\n",
      "Good-to-Bad ratio: 0.5508745996550874\n"
     ]
    }
   ],
   "source": [
    "for_cm_df = pd.DataFrame(confusion_matrix(ytest, y_test_forest))\n",
    "for_cm_df.columns = ['No Default Predicted', 'Default Predicted']\n",
    "for_cm_df['ind'] = ['No Default Occurred', 'Default Occurred']\n",
    "for_cm_df = for_cm_df.set_index('ind')\n",
    "for_cm_df.index.name = None\n",
    "print('Confusion matrix: \\n\\n{}'.format(for_cm_df))\n",
    "gb_ratio = for_cm_df.loc['No Default Occurred', 'Default Predicted'] / for_cm_df.loc['Default Occurred', 'Default Predicted']\n",
    "print('\\nGood-to-Bad ratio: {}'.format(gb_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models are quite good at predicting defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, macroeconomic features are imported. Bankruptcy data that was drafted for an earlier analysis is pulled in from a .pkl file, which is available in the Google Drive location linked below. The remainder of the data was sourced from the Federal Reserve Economic Data (FRED). The API_KEY variable below has been replaced with an empty string so that the private API key is not shared.\n",
    "\n",
    "BK Data .pkl: https://drive.google.com/open?id=1hZc5-S451fGmIY1s-D2IEqVg_rTcsRzv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the .pkl file containing the bankruptcy data\n",
    "bk_data = pd.read_pickle(r'C:\\Users\\Mark\\Desktop\\springboard_projects\\data\\monthly_bk_data.pkl')\n",
    "\n",
    "# Convert BKs to numeric\n",
    "bk_data['monthly_bks'] = pd.to_numeric(bk_data['monthly_bks'], errors='coerce')\n",
    "bk_data = bk_data.reset_index()\n",
    "bk_data.columns = ['date', 'bks']\n",
    "\n",
    "# Pull in data using the FRED API\n",
    "# Set up the API. The API key is removed from the published version.\n",
    "API_KEY = ''\n",
    "fred = fredapi.Fred(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the 3 features already included in the data previously imported, a number of new features will be incorporated and evaluated. All of them are sourced from the FRED (Federal Reserve Economic Database) using the fredapi library and adjusted to only end-of-month values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TED spread\n",
    "ted_spread = fred.get_series('TEDRATE').to_frame().reset_index()\n",
    "ted_spread.columns = ['date', 'ted_spread']\n",
    "ted_spread['date'] = ted_spread['date'].apply(lambda x: x.replace(day=1))\n",
    "ted_spread = ted_spread.groupby([ted_spread['date']]).mean()\n",
    "\n",
    "# St. Louis Fed Financial Stress Index\n",
    "financial_stress = fred.get_series('STLFSI').to_frame().reset_index()\n",
    "financial_stress.columns = ['date', 'financial_stress']\n",
    "financial_stress['date'] = financial_stress['date'].apply(lambda x: x.replace(day=1))\n",
    "financial_stress = financial_stress.groupby([financial_stress['date']]).mean()\n",
    "\n",
    "# Civilian labor force participation rate\n",
    "participation_rate = fred.get_series('LNS11300060').to_frame().reset_index()\n",
    "participation_rate.columns = ['date', 'participation_rate']\n",
    "\n",
    "# Total vehicle sales\n",
    "vehicle_sales = fred.get_series('TOTALSA').to_frame().reset_index()\n",
    "vehicle_sales.columns = ['date', 'vehicle_sales']\n",
    "# Make the variable the month-over-month change instead of the in-period value.\n",
    "vehicle_sales['vehicle_sales'] = (vehicle_sales['vehicle_sales'] - vehicle_sales['vehicle_sales'].shift(1)) / vehicle_sales['vehicle_sales'].shift(1)\n",
    "\n",
    "# Consumer Sentiment\n",
    "consumer_sentiment = fred.get_series('UMCSENT').to_frame().reset_index()\n",
    "consumer_sentiment.columns = ['date', 'consumer_sentiment']\n",
    "\n",
    "# Unemployment Rate\n",
    "unemployment_data = fred.get_series('UNRATE').to_frame().reset_index()\n",
    "unemployment_data.columns = ['date', 'unemployment_rate']\n",
    "\n",
    "# S&P 500 Data\n",
    "sp_data = fred.get_series('SP500').to_frame().reset_index()\n",
    "sp_data.columns = ['date', 'sp']\n",
    "sp_data['date'] = sp_data['date'].apply(lambda x: x.replace(day=1))\n",
    "sp_data = sp_data.groupby([sp_data['date']]).mean()\n",
    "# Make the variable the month-over-month change instead of the in-period value.\n",
    "sp_data['sp'] = (sp_data['sp'] - sp_data['sp'].shift(1)) / sp_data['sp'].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are merged into the Lending Club data so that they can be used as features in the Machine Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge the new features into the LC data\n",
    "lc_data = lc_data.merge(right=ted_spread, left_on='orig_month', right_on='date', how='left', suffixes=('', ''))\n",
    "if 'date' in lc_data.columns:\n",
    "    lc_data = lc_data.drop(['date'], axis=1)\n",
    "lc_data = lc_data.merge(right=financial_stress, left_on='orig_month', right_on='date', how='left', suffixes=('', ''))\n",
    "if 'date' in lc_data.columns:\n",
    "    lc_data = lc_data.drop(['date'], axis=1)\n",
    "lc_data = lc_data.merge(right=participation_rate, left_on='orig_month', right_on='date', how='left', suffixes=('', ''))\n",
    "if 'date' in lc_data.columns:\n",
    "    lc_data = lc_data.drop(['date'], axis=1)\n",
    "lc_data = lc_data.merge(right=vehicle_sales, left_on='orig_month', right_on='date', how='left', suffixes=('', ''))\n",
    "if 'date' in lc_data.columns:\n",
    "    lc_data = lc_data.drop(['date'], axis=1)\n",
    "lc_data = lc_data.merge(right=consumer_sentiment, left_on='orig_month', right_on='date', how='left', suffixes=('', ''))\n",
    "if 'date' in lc_data.columns:\n",
    "    lc_data = lc_data.drop(['date'], axis=1)\n",
    "lc_data = lc_data.merge(right=unemployment_data, left_on='orig_month', right_on='date', how='left', suffixes=('', ''))\n",
    "if 'date' in lc_data.columns:\n",
    "    lc_data = lc_data.drop(['date'], axis=1)\n",
    "lc_data = lc_data.merge(right=sp_data, left_on='orig_month', right_on='date', how='left', suffixes=('', ''))\n",
    "if 'date' in lc_data.columns:\n",
    "    lc_data = lc_data.drop(['date'], axis=1)\n",
    "lc_data = lc_data.merge(right=bk_data, left_on='orig_month', right_on='date', how='left', suffixes=('', ''))\n",
    "if 'date' in lc_data.columns:\n",
    "    lc_data = lc_data.drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Run Machine Learning Models with Macroeconomic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same steps that were previously performed to ready the data for ML applications is performed below. First, all object-type variables are split into dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_data = pd.get_dummies(lc_data, columns=list(lc_data.select_dtypes(include=[\"object\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is again split into predictive features (X) and the outcome variable (y) and split into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lc_data.drop(['charge_off_flag', 'orig_month'], axis=1).values\n",
    "y = lc_data['charge_off_flag'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = preprocessing.StandardScaler().fit_transform(Xtrain)\n",
    "Xtest = preprocessing.StandardScaler().fit_transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is performed on the modified dataset, incorporating the macroeconomic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN training...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 1154.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on the train set...\n",
      "Predicting on the test set...\n"
     ]
    }
   ],
   "source": [
    "knn_parameters = {\"n_neighbors\": [3, 5], \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\"], \n",
    "               \"weights\": [\"uniform\", \"distance\"]}\n",
    "clf = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=knn_parameters, cv=3, n_jobs=-1, \n",
    "                   scoring=\"f1\", verbose=10)\n",
    "\n",
    "print('Starting KNN training...')\n",
    "clf.fit(Xtrain, ytrain)\n",
    "print('Predicting on the train set...')\n",
    "y_val_knn = clf.predict(Xtrain)\n",
    "print('Predicting on the test set...')\n",
    "y_test_knn = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very slightly better for KNN - the good-to-bad ratio went from .93 to .92."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "\n",
      "                     No Default Predicted  Default Predicted\n",
      "No Default Occurred                 70167               3749\n",
      "Default Occurred                     8546               4079\n",
      "\n",
      "Good-to-Bad ratio: 0.9190978180926698\n"
     ]
    }
   ],
   "source": [
    "knn_m_cm_df = pd.DataFrame(confusion_matrix(ytest, y_test_knn))\n",
    "knn_m_cm_df.columns = ['No Default Predicted', 'Default Predicted']\n",
    "knn_m_cm_df['ind'] = ['No Default Occurred', 'Default Occurred']\n",
    "knn_m_cm_df = knn_m_cm_df.set_index('ind')\n",
    "knn_m_cm_df.index.name = None\n",
    "print('Confusion matrix: \\n\\n{}'.format(knn_m_cm_df))\n",
    "gb_ratio = knn_m_cm_df.loc['No Default Occurred', 'Default Predicted'] / knn_m_cm_df.loc['Default Occurred', 'Default Predicted']\n",
    "print('\\nGood-to-Bad ratio: {}'.format(gb_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, SVM is performed on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVM training...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 64.2min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 97.1min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 169.7min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed: 296.0min remaining: 59.2min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed: 436.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on the train set...\n",
      "Predicting on the test set...\n"
     ]
    }
   ],
   "source": [
    "svc_parameters = {\"kernel\": [\"rbf\"], \"gamma\": [0.001, 0.0001], \"C\": [1, 10, 100, 1000]}\n",
    "clf = GridSearchCV(estimator=SVC(), param_grid=svc_parameters, cv=3, n_jobs=-1, \n",
    "                   scoring=\"f1\", verbose=10)\n",
    "\n",
    "print('Starting SVM training...')\n",
    "clf.fit(Xtrain, ytrain)\n",
    "print('Predicting on the train set...')\n",
    "y_val_svm = clf.predict(Xtrain)\n",
    "print('Predicting on the test set...')\n",
    "y_test_svm = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for SVM also improved very slightly, from .53 to .52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "\n",
      "                     No Default Predicted  Default Predicted\n",
      "No Default Occurred                 71486               2430\n",
      "Default Occurred                     7927               4698\n",
      "\n",
      "Good-to-Bad ratio: 0.5172413793103449\n"
     ]
    }
   ],
   "source": [
    "svm_m_cm_df = pd.DataFrame(confusion_matrix(ytest, y_test_svm))\n",
    "svm_m_cm_df.columns = ['No Default Predicted', 'Default Predicted']\n",
    "svm_m_cm_df['ind'] = ['No Default Occurred', 'Default Occurred']\n",
    "svm_m_cm_df = svm_m_cm_df.set_index('ind')\n",
    "svm_m_cm_df.index.name = None\n",
    "print('Confusion matrix: \\n\\n{}'.format(svm_m_cm_df))\n",
    "gb_ratio = svm_m_cm_df.loc['No Default Occurred', 'Default Predicted'] / svm_m_cm_df.loc['Default Occurred', 'Default Predicted']\n",
    "print('\\nGood-to-Bad ratio: {}'.format(gb_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the Random Forest model is re-run on the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest training\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   29.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   45.0s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed:  1.2min remaining:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on the train set...\n",
      "Predicting on the test set...\n"
     ]
    }
   ],
   "source": [
    "forest_parameters = {\"n_estimators\": [5, 10, 50], \"max_depth\": [2, 5, 7, 9]}\n",
    "clf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=forest_parameters, cv=3, n_jobs=-1, \n",
    "                   scoring=\"f1\", verbose=10)\n",
    "\n",
    "print('Starting Random Forest training')\n",
    "clf.fit(Xtrain, ytrain)\n",
    "print('Predicting on the train set...')\n",
    "y_val_forest = clf.predict(Xtrain)\n",
    "print('Predicting on the test set...')\n",
    "y_test_forest = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for Random Forest are very good - the model improved from having a good-to-bad ratio of .55 to one of .49, the lowest of any prediction by a small but significant margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "\n",
      "                     No Default Predicted  Default Predicted\n",
      "No Default Occurred                 71713               2203\n",
      "Default Occurred                     8138               4487\n",
      "\n",
      "Good-to-Bad ratio: 0.49097392467127254\n"
     ]
    }
   ],
   "source": [
    "rf_m_cm_df = pd.DataFrame(confusion_matrix(ytest, y_test_forest))\n",
    "rf_m_cm_df.columns = ['No Default Predicted', 'Default Predicted']\n",
    "rf_m_cm_df['ind'] = ['No Default Occurred', 'Default Occurred']\n",
    "rf_m_cm_df = rf_m_cm_df.set_index('ind')\n",
    "rf_m_cm_df.index.name = None\n",
    "print('Confusion matrix: \\n\\n{}'.format(rf_m_cm_df))\n",
    "gb_ratio = rf_m_cm_df.loc['No Default Occurred', 'Default Predicted'] / rf_m_cm_df.loc['Default Occurred', 'Default Predicted']\n",
    "print('\\nGood-to-Bad ratio: {}'.format(gb_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the methods tested showed an improvement with incorporation of macroeconomic data - for the time period tested, at least, incorporating macroeconomic variables into decision-making would have resulted in improvements in loan performance. The magnitude of the improvements would be quite large: 4,487 bad loans would have never been made, at the cost of only 2,203 good loans, which could easily represent a 7-figure cost savings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
